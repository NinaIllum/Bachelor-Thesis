---
title: 'Code: Factor Analyis and GGM'
author: "Nina Illum"
date: "18/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Basic packages}

# install necessary packages
#install.packages("")

# load basic packages
library(tidyverse)

# load packages for factor analysis
library(corpcor)
library(GPArotation)
library(psych)
library(paran)
library(lavaan)
library(polycor)

# load packages for network models
library("psychonetrics")
library("qgraph")
library("ggplot2")
library("bootnet")
library("Rmpfr")
library("semPlot")
#library(ggplot2)
library(ggpubr)


```


```{r Load data}

# load anonymized data
data_final <- read_csv("data//data_anonymous.csv") %>%
  as.tibble()

# change id from character to integer
#data_final$id <- data_final$id %>% 
#  as.factor() %>% 
#  as.integer()

```

```{r Subdata}

# Inner Speech questions data
inner_speech_full <- data_final[c(5:39)]
# Inner Speech questions data with only the 26 items found by Alderson-Day et al. (2018)
inner_speech <- data_final[c(5:24, 29, 31:32, 35:36, 39)]
# Inner Speech questions data with ID
inner_speech_ID <- data_final[c(2,5:24, 29, 31:32, 35:36, 39)]

# Rumination questions data
rumination <- data_final[c(40:61)]

# Depression questions data fro Polychoric factanal()
depression <- data_final[c(62:70)]
# Extra dataframe for Polychoric fa()
depression_Polychoric <- data_final[c(62:70)]
# Extra dataframe for Bartlett factanal()
depression_Bartlett <- data_final[c(62:70)]
# Extra dataframe for Pearson + Bartlett fa()
depression_Pearson <- data_final[c(62:70)]

```






## Data Screening

### Inner speech

```{r Correlation Estimates + Parallel Analysis (IS)}

# Kaiser-Meyer-Olkin measure (KMO)
KMO(inner_speech)

# Bartlett's test of sphericity
cortest.bartlett(inner_speech)


# create covariance matrix
cov_matrix <- cov(inner_speech)

# Horn's Parallel Analysis of Principal Components/Factors
paran(inner_speech, graph = T, iterations = 100)

```

### Rumination

```{r Correlation Estimates + Parallel Analysis (Rumination)}

# Kaiser-Meyer-Olkin measure (KMO)
KMO(inner_speech)

# Bartlett's test of sphericity
cortest.bartlett(inner_speech)


# create covariance matrix
cov_matrix <- cov(inner_speech)

# Horn's Parallel Analysis of Principal Components/Factors
paran(inner_speech, graph = T, iterations = 100)

```






## Factor Analysis

```{r Inner Speech Factor analysis (POLYCHORIC + factanal())}
# scree plot
paran(inner_speech, graph = T) # looks like five factors

## polychoric correlation matrix
# divide likert-scores because polychoric() only allows max 8 categories per item
inner_speech_max8 <- inner_speech / 2


# run polychoric correlations on inner speech data
polycor_IS <- cor_auto(inner_speech_max8)

# alternative method
#poly_values_IS <- polychoric(inner_speech_max8)
# extracts and saves polychoric corrs as data frame
#polycor_IS <- poly_values_IS$rho


# save FA analysis with number of factors from paran()
## factor analysis: Assumes ordinal data
fa_IS <- factanal(factors = 5, covmat = polycor_IS, rotation = 'oblimin')

# look at the items in the right order
print(fa_IS, cutoff = .27, sort = F)
```

```{r Inner Speech Factor analysis (POLYCHORIC in fa())}

## factor analysis: Assumes ordinal data and performs polychoric correlations
fa_IS_Polychoric <- fa(inner_speech_max8, nfactors = 5, rotate = "oblimin", scores = "Bartlett", cor = "poly")

print(fa_IS_Polychoric$loadings, cutoff = .27, sort = F)

fa_IS_Polychoric$scores

# save factor scores in original dataframe (only with standard factanal() and Pearson correlations)
depression_Polychoric$is_f1 <- fa_IS_Polychoric$scores[,"MR2"]
depression_Polychoric$is_f2 <- fa_IS_Polychoric$scores[,"MR1"]
depression_Polychoric$is_f3 <- fa_IS_Polychoric$scores[,"MR5"]
depression_Polychoric$is_f4 <- fa_IS_Polychoric$scores[,"MR3"]
depression_Polychoric$is_f5 <- fa_IS_Polychoric$scores[,"MR4"]

```


```{r Inner Speech Factor analysis (BARTLETT in factanal())}
## assumes continuous data
fa_IS_Bartlett <- factanal(inner_speech, factors = 5, rotation = 'oblimin', scores = "Bartlett") # different types of scores exist

# look at the items in the right order
print(fa_IS_Bartlett, cutoff = .27, sort = F)

# save factor scores in original dataframe (only with standard factanal() and Pearson correlations)
depression_Bartlett$is_f1 <- fa_IS_Bartlett$scores[,"Factor1"]
depression_Bartlett$is_f2 <- fa_IS_Bartlett$scores[,"Factor2"]
depression_Bartlett$is_f3 <- fa_IS_Bartlett$scores[,"Factor3"]
depression_Bartlett$is_f4 <- fa_IS_Bartlett$scores[,"Factor4"]
depression_Bartlett$is_f5 <- fa_IS_Bartlett$scores[,"Factor5"]

```


```{r Inner Speech Factor analysis (PEARSON in fa())}
### Alternative function

fa_IS_Pearson <- fa(inner_speech, nfactors = 5, rotate = "oblimin", scores = "Bartlett", cor = "cor")

# print factor loadings
print(fa_IS_Pearson$loadings, cutoff = .27, sort = F)

fa_IS_Pearson$scores

depression_Pearson$is_f1 <- fa_IS_Pearson$scores[,"MR2"]
depression_Pearson$is_f2 <- fa_IS_Pearson$scores[,"MR3"]
depression_Pearson$is_f3 <- fa_IS_Pearson$scores[,"MR5"]
depression_Pearson$is_f4 <- fa_IS_Pearson$scores[,"MR1"]
depression_Pearson$is_f5 <- fa_IS_Pearson$scores[,"MR4"]

```


```{r Inner Speech Calculate New Scores (POLYCHORIC)}

# calculate factor scores for each Inner Speech factor
is_score_f1 <- (data_final$IS_1 * fa_IS$loadings[1,1] + data_final$IS_7 * fa_IS$loadings[7,1] + data_final$IS_8 * fa_IS$loadings[8,1] + data_final$IS_14 * fa_IS$loadings[14,1] + data_final$IS_15 * fa_IS$loadings[15,1]) / 5

is_score_f2 <- (data_final$IS_20 * fa_IS$loadings[20,2] + data_final$IS_28 * fa_IS$loadings[23,2] + data_final$IS_31 * fa_IS$loadings[24,2] + data_final$IS_35 * fa_IS$loadings[26,2]) / 4

is_score_f3 <- (data_final$IS_3 * fa_IS$loadings[3,3] + data_final$IS_4 * fa_IS$loadings[4,3] + data_final$IS_5 * fa_IS$loadings[5,3] + data_final$IS_12 * fa_IS$loadings[12,3] + data_final$IS_16 * fa_IS$loadings[16,3]) / 5

is_score_f4 <- (data_final$IS_9 * fa_IS$loadings[9,4] + data_final$IS_11 * fa_IS$loadings[11,4] + data_final$IS_17 * fa_IS$loadings[17,4] + data_final$IS_18 * fa_IS$loadings[18,4] + data_final$IS_19 * fa_IS$loadings[19,4] + data_final$IS_27 * fa_IS$loadings[22,4] + data_final$IS_32 * fa_IS$loadings[25,4]) / 7

is_score_f5 <- (data_final$IS_2 * fa_IS$loadings[2,5] + data_final$IS_6 * fa_IS$loadings[6,5] + data_final$IS_10 * fa_IS$loadings[10,5] + data_final$IS_13 * fa_IS$loadings[13,5] + data_final$IS_25 * fa_IS$loadings[21,5]) / 5


# save factor scores in depression data frame
depression$is_f1 <- is_score_f1
depression$is_f2 <- is_score_f2
depression$is_f3 <- is_score_f3
depression$is_f4 <- is_score_f4
depression$is_f5 <- is_score_f5

```



```{r Rumination Factor analysis}
# scree plot
paran(rumination, graph = T) # looks like five factors

# divide likert-scores because polychoric() only allows max 8 categories per item
rumination_max8 <- rumination / 3

# run polychoric correlations on inner speech data
poly_values_R <- polychoric(rumination_max8)

# extracts and saves polychoric corrs as data frame
polycor_R <- poly_values_R$rho


# save FA analysis with number of factors from paran()
## assumes ordinal data
fa_R <- factanal(factors = 3, covmat = polycor_R, rotation='oblimin')

# look at the items in the right order
print(fa_R, cutoff = .3, sort = F)
```


```{r Rumination Factor analysis (POLYCHORIC + factanal())}


# calculate factor scores for each Inner Speech factor
r_score_f1 <- (data_final$R_1 * fa_R$loadings[1,1] + data_final$R_2 * fa_R$loadings[2,1] + data_final$R_3 * fa_R$loadings[3,1] + data_final$R_5 * fa_R$loadings[5,1] + data_final$R_9 * fa_R$loadings[9,1] + data_final$R_10 * fa_R$loadings[10,1] + data_final$R_13 * fa_R$loadings[13,1] + data_final$R_15 * fa_R$loadings[15,1] + data_final$R_16 * fa_R$loadings[16,1] + data_final$R_17 * fa_R$loadings[17,1] + data_final$R_18 * fa_R$loadings[18,1] + data_final$R_19 * fa_R$loadings[19,1] + data_final$R_20 * fa_R$loadings[20,1] + data_final$R_22 * fa_R$loadings[22,1]) / 14

r_score_f2 <- (data_final$R_4 * fa_R$loadings[4,2] + data_final$R_6 * fa_R$loadings[6,2] + data_final$R_8 * fa_R$loadings[8,2] + data_final$R_14 * fa_R$loadings[14,2]) / 4

r_score_f3 <- (data_final$R_7 * fa_R$loadings[7,3] + data_final$R_11 * fa_R$loadings[11,3] + data_final$R_12 * fa_R$loadings[12,3] + data_final$R_21 * fa_R$loadings[21,3]) / 4


# save factor scores in depression data frame
depression$r_f1 <- r_score_f1
depression$r_f2 <- r_score_f2
depression$r_f3 <- r_score_f3

```


```{r Rumination Factor analysis (POLYCHORIC in fa())}

### Alternative fa() function
fa_R_Polychoric <- fa(rumination_max8, nfactors = 3, rotate = "oblimin", scores = "Bartlett", cor = "poly")

# print factor loadings
print(fa_R_Polychoric$loadings, cutoff = .3, sort = F)

fa_R_Polychoric$scores

depression_Polychoric$r_f1 <- fa_R_Polychoric$scores[,"MR1"]
depression_Polychoric$r_f2 <- fa_R_Polychoric$scores[,"MR3"]
depression_Polychoric$r_f3 <- fa_R_Polychoric$scores[,"MR2"]

```


```{r Rumination Factor analysis (BARTLETT)}


## assumes continuous data
fa_R_Bartlett <- factanal(rumination, factors = 3, rotation = 'oblimin', scores = "Bartlett") # different types of scores exist

# look at the items in the right order
print(fa_R_Bartlett, cutoff = .27, sort = F)

# save factor scores in original dataframe (only with standard factanal() and Pearson correlations)
depression_Bartlett$r_f1 <- fa_R_Bartlett$scores[,"Factor1"]
depression_Bartlett$r_f2 <- fa_R_Bartlett$scores[,"Factor2"]
depression_Bartlett$r_f3 <- fa_R_Bartlett$scores[,"Factor3"]

# look at how e.g. gender predicts scores on a specific factor
#summary(lm(is_f3 ~ gender, data_final))


```

```{r Rumination Factor analysis (PEARSON + BARTLETT)}

### Alternative fa() function
fa_R_Pearson <- fa(rumination, nfactors = 3, rotate = "oblimin", scores = "Bartlett", cor = "cor")

# print factor loadings
print(fa_R_Pearson$loadings, cutoff = .25, sort = F)

fa_R_Pearson$scores

depression_Pearson$r_f1 <- fa_R_Pearson$scores[,"MR1"]
depression_Pearson$r_f2 <- fa_R_Pearson$scores[,"MR3"]
depression_Pearson$r_f3 <- fa_R_Pearson$scores[,"MR2"]

```








## Network models

## Network Model Estimations

```{r Groups and names}

group2_D <- c(1:9) # depression items
#"D_1", "D_2", "D_3", "D_4", "D_5", "D_6", "D_7", "D_8", "D_9")

group2_IS <- c(10:14) # inner speech items
#"is_f1"   "is_f2"  "is_f3"   "is_f4"   "is_f5"

group2_R <- c(15:17) # rumination items
#"r_f1"    "r_f2"    "r_f3"

# list questionnaire groups
q_groups2 <- list(group2_IS, group2_R, group2_D)

# labels for nodes in the plot
node_labels <- c("D1", "D2", "D3", "D4", "D5", "D6", "D7", "D8", "D9", "IS1", "IS2", "IS3", "IS4", "IS5", "R1", "R2", "R3")

# real names for nodes in the plot
node_names <- c("Little interest/pleasure", "Feeling down", "Sleep troubles", "Low energy", "Appetite troubles", "Feeling bad about yourself", "Concentration troubles", "Behavioral tempo troubles", "Self-harm or suicidal thoughts", "Condensed", "Evaluative/critical", "Other people", "Positive/regulatory", "Dialogic", "Brooding", "Depression-Related", "Reflection")

```


```{r Network Estimation (POLYCHORIC CORRELATIONS)}

# estimate network structure
Network <- estimateNetwork(depression, default = "ggmModSelect")

# review edges
Network$results

# plot network
plot(Network,
     layout = "spring",
     theme = "colorblind",
     labels = node_labels,
     label.scale = T,
     label.font = 6,
     groups = q_groups2,
     legend = T,
     legend.mode = "names",
     nodeNames = node_names,
     legend.cex = 0.4,
     GLratio = 3,
     title = "Assumption of ordinal data - factanal()")

```

```{r Network Estimation (POLYCHORIC in fa())}

# estimate network structure
Network_Polychoric <- estimateNetwork(depression_Polychoric, default = "ggmModSelect")

# review edges
Network_Polychoric$results

# plot network
plot(Network_Polychoric,
     layout = "spring",
     theme = "colorblind",
     labels = node_labels,
     label.scale = T,
     label.font = 6,
     groups = q_groups2,
     legend = T,
     legend.mode = "names",
     nodeNames = node_names,
     legend.cex = 0.4,
     GLratio = 3,
     title = "Assumption of ordinal data w. fa()")

```

```{r Network Estimation (BARTLETT SCORES)}

# estimate network structure
Network_Bartlett <- estimateNetwork(depression_Bartlett, default = "ggmModSelect")

# review edges
Network_Bartlett$results

# plot network
plot(Network_Bartlett,
     layout = "spring",
     theme = "colorblind",
     labels = node_labels,
     label.scale = T,
     label.font = 6,
     groups = q_groups2,
     legend = T,
     legend.mode = "names",
     nodeNames = node_names,
     legend.cex = 0.4,
     GLratio = 3,
     title = "Assumption of continuous data")

```

```{r Network Estimation (PEARSON CORRELATIONS)}

# estimate network structure
Network_Pearson <- estimateNetwork(depression_Pearson, default = "ggmModSelect")

# review edges
Network_Pearson$results

# plot network
plot(Network_Pearson,
     layout = "spring",
     theme = "colorblind",
     labels = node_labels,
     label.scale = T,
     label.font = 6,
     groups = q_groups2,
     legend = T,
     legend.mode = "names",
     nodeNames = node_names,
     legend.cex = 0.4,
     GLratio = 3,
     title = "Assumption of continuous data (Pearson)")

```


```{r Network Estimation without IS1 node (nonode)}

depression_nonode <- depression[, c("D_1", "D_2", "D_3", "D_4", "D_5", "D_6", "D_7", "D_8", "D_9", "is_f2", "is_f3", "is_f4", "is_f5", "r_f1", "r_f2", "r_f3")]

# estimate network structure
Network_nonode <- estimateNetwork(depression_nonode, default = "ggmModSelect")

# review edges
Network_nonode$results



### PLOTTING

# create groups for each questionnaire
group2_D_nonode <- c(1:9)
#"D_1", "D_2", "D_3", "D_4", "D_5", "D_6", "D_7", "D_8", "D_9")

group2_IS_nonode <- c(10:13)
#"is_f2"  "is_f3"   "is_f4"   "is_f5"

group2_R_nonode <- c(14:16)
#"r_f1"    "r_f2"    "r_f3"

# list questionnaire groups
q_groups2_nonode <- list(group2_IS_nonode, group2_R_nonode, group2_D_nonode)

# labels without IS1 node
node_labels_nonode <- c("D1", "D2", "D3", "D4", "D5", "D6", "D7", "D8", "D9", "IS2", "IS3", "IS4", "IS5", "R1", "R2", "R3")

# plot network (just to check)
#plot(Network_nonode,
#     layout = "spring",
#     labels = node_labels_nonode,
#     groups = q_groups2_nonode,
#     theme = "colorblind")


```


```{r Computing centrality indices}

# plot centrality (factanal() network)
#centralityPlot(Network, include = c("Strength", "Betweenness", "Closeness"))

```


```{r Computing centrality indices}

# plot centrality (fa() network)
centralityPlot(Network_Polychoric, include = c("Strength", "Betweenness", "Closeness"))

### NONODE
# plot centrality without Inner Speech item 5 (Condensed speech)
#centralityPlot(Network_nonode, include = c("Strength", "Betweenness", "Closeness"))

```



```{r Edge-weight accuracy}

# bootstrap method
boot1 <- bootnet(Network_Polychoric, nBoots = 2500, nCores = 8) # OBS!! Takes a while to run (min. 1 hour)
# standard nBoots = 1000, Epskamp recommendation = 2500

# print overview with characteristics of the sample network
print(boot1)

# plot bootstrapped CIs for estimated edge parameters
edge_plot <- plot(boot1, labels = FALSE, order = "sample")

```


```{r Centrality stability}

# estimatingbased on data subsets (case-dropping bootstrap)
boot2 <- bootnet(Network_Polychoric, nBoots = 50, type = "case", nCores = 8, statistics = c("edge", "strength", "closeness", "betweenness")) # OBS!! Takes a while to run
# standard nBoots = 1000, Epskamp recommendation = 2500

summary(boot2, statistics = c("edge", "intercept","strength", "closeness", "betweenness", "distance"), perNode = F, rank = F)

# plot centrality stability under subsetting
plot(boot2)

plot(boot2, c("strength", "betweenness", "closeness")) # 'closeness' does not work because the is_f1 node is not connected to any other nodes
  
# calculate CS-coefficient
CS_coef <- corStability(boot2)
print(CS_coef)

```



```{r Centrality stability without IS1 node (nonode)}

# estimating based on data subsets (case-dropping bootstrap)
boot2_nonode <- bootnet(Network_nonode, nBoots = 50, type = "case", nCores = 8, statistics = c("edge", "strength", "closeness", "betweenness")) # OBS!! Takes a while to run
# standard nBoots = 1000, Epskamp recommendation = 2500

# plot centrality stability under subsetting
plot(boot2_nonode)

plot(boot2_nonode, c("strength", "betweenness", "closeness"))

# calculate CS-coefficient
#corStability(boot2)

#summary(boot2, statistics = c("edge", "intercept","strength", "closeness", "betweenness", "distance"), perNode = F, rank = F)
summary(boot2_nonode)

# calculate CS-coefficient
CS_coef_nonode <- corStability(boot2_nonode)
print(CS_coef_nonode)

```

```{r Plot Centrality Stability estimates}

# define plots for each centrality estimates
str_plot <- plot(boot2, "strength")
bet_plot <- plot(boot2, "betweenness")
clo_plot <- plot(boot2_nonode, "closeness")

# plot grid with the 3 centrality estimates
central_plot <- ggarrange(str_plot, bet_plot, clo_plot, nrow = 1)

# plot grid with edge weight accuracy and centrality estimates
ggarrange(edge_plot, central_plot, nrow = 2, heights = c(2,1))

```



```{r Testing for significant differences}

# e.g.testing if node 3 and 17 differ in node strength centrality
differenceTest(boot1, 3, 17, "strength")

## plot difference tests between all pairs of edges and centrality indices
# e.g. node strength centrality
plot(boot1, "edge", plot = "difference", onlyNonZero = TRUE, order = "sample")

```














